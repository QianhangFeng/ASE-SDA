exp_name: 'val_1' # baseline
device: "cuda:0" # watch -n 0.5 nvidia-smi
seed: 42

audio_args:
  sr: 32000
  n_fft: 1024
  hop_length: 320
  f_min: 50
  f_max: 14000
  n_mels: 64
  max_length: 0 # set to 10 for HTSAT encoder, and set to 0 or 30 for CNN encoder
  mono: True

  train_audiocaps_wav_path: "/home/feng/desktop/data/audio/audiocaps/train"
  train_clotho_wav_path: "/home/feng/desktop/data/audio/clotho/development"
  val_audiocaps_wav_path: "/home/feng/desktop/data/audio/audiocaps/val"
  val_clotho_wav_path: "/home/feng/desktop/data/audio/clotho/validation"
  test_audiocaps_wav_path: "/home/feng/desktop/data/audio/audiocaps/test"
  test_clotho_wav_path: "/home/feng/desktop/data/audio/clotho/evaluation"

  train_wavcaps_wav_path: "/home/feng/desktop/data/audio/wavcaps/"

  train_audiocaps_csv_path: "/home/feng/desktop/data/caption/audiocaps/train.csv"
  train_clotho_csv_path: "/home/feng/desktop/data/caption/clotho/clotho_captions_development.csv"
  val_audiocaps_csv_path: "/home/feng/desktop/data/caption/audiocaps/val.csv"
  val_clotho_csv_path: "/home/feng/desktop/data/caption/clotho/clotho_captions_validation.csv"
  test_audiocaps_csv_path: "/home/feng/desktop/data/caption/audiocaps/test.csv"
  test_clotho_csv_path: "/home/feng/desktop/data/caption/clotho/clotho_captions_evaluation.csv"

  train_wavcaps_json_path_as: "/home/feng/desktop/data/caption/wavcaps/as_final.json"
  train_wavcaps_json_path_bbc: "/home/feng/desktop/data/caption/wavcaps/bbc_final.json"
  train_wavcaps_json_path_fsd: "/home/feng/desktop/data/caption/wavcaps/fsd_final.json"
  train_wavcaps_json_path_sb: "/home/feng/desktop/data/caption/wavcaps/sb_final.json"

data_args:
  dataset: "Clotho"
  fn_train: True
  batch_size: 8
  num_workers: 8
  total_size: 7464


audio_encoder_args:
  model_arch: "convnext" # EfficinetAT, cnn, CED, convnext
  model_name: "convnext" # dymn10_as, Cnn14, mispeech/ced-base, convnext
  pretrained: true
  freeze: false
  spec_augment: true

token_encoder_args:
  hidden_size: 768 # 768
  seq_len: 94

text_decoder_args:
  name: "facebook/bart-base" # facebook/bart-base, llama
  pretrained: false # true
  hidden_size: 768
  pad_token_id: 1
  bos_token_id: 0
  eos_token_id: 2
  decoder_start_token_id: 2


optim_args:
  scheduler: "cosine"
  lr: !!float 4e-6
  warmup_steps: 6400
  optimizer_name: "adamw"
  betas: [0.9, 0.999]
  eps: !!float 1e-8
  momentum: 0.9
  gamma: 0.1
  warmup_epochs: 2
  step_epochs: 10
  weight_decay: !!float 1e-6


training:
  epochs: 100
  clip_grad: 2
  dropout: 0.2